{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Processing Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf # 2.3.0\n\nimport librosa\nfrom pathlib import Path\nimport pickle\n\ntf.random.set_seed(\n    1234\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SHUFFLE_SEED = 43\nVALID_SPLIT = 0.1\nSAMPLING_RATE = 16000\nBATCH_SIZE = 32 #128\nDURATION = 5.0 #20 SECONDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET_AUDIO_PATH = '../your/pathto/train_audio'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def has_enough_samples(audio_path):\n    \"\"\" Check if audio has enough samples if audio has exact duration \"\"\"\n    audio, y = librosa.load(audio_path, sr=16000)\n    return len(audio) >= (SAMPLING_RATE * DURATION)\n\ndef is_long_audio(audio_path, threshold=DURATION):\n    \"\"\" Check long audios from dataset by threshold \"\"\"\n    duration = librosa.get_duration(filename=audio_path)\n    if duration > threshold:\n        return True\n    elif (duration == threshold) and has_enough_samples(audio_path):\n        return True\n    return False\n\ndef get_audio_metadata():\n    \"\"\" Get audio paths and labels from directory metadata \"\"\"\n    audio_paths = []\n    labels = []\n    for label, name in enumerate(class_names):\n        print(f\"Processing class {name}\")\n        dir_path = Path(DATASET_AUDIO_PATH) / name\n        class_sample_paths = [\n                             os.path.join(dir_path, filepath)\n                             for filepath in os.listdir(dir_path)\n                             if filepath.endswith(\".mp3\") and\n                             is_long_audio(os.path.join(dir_path, filepath))\n                             ]\n        audio_paths += class_sample_paths\n        labels += [label] * len(class_sample_paths)\n\n    print(\n        f\"Found {len(audio_paths)} files belonging to {len(class_names)} classes.\"\n        )\n    return audio_paths, labels\n\ndef tf_decode_mp3(mp3_path):\n    \"\"\" Describe the return shapes and types of decode_mp3 function \"\"\"\n    audio_shape = [int(DURATION * SAMPLING_RATE), 1]\n    [audio,] = tf.py_function(decode_mp3, [mp3_path], [tf.float32])\n    audio.set_shape(audio_shape)\n    return audio\n\ndef decode_mp3(mp3_path):\n    \"\"\"Reads and decodes an mp3 audio file.\"\"\"\n    mp3_path = mp3_path.numpy().decode(\"utf-8\")\n    audio, sr = librosa.load(mp3_path, duration=DURATION, sr=SAMPLING_RATE) # 5*16000 = 80000 SAMPLES\n    return np.expand_dims(audio, axis=1)\n\ndef paths_and_labels_to_dataset(audio_paths, labels):\n    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n    audio_ds = path_ds.map(tf_decode_mp3)\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    return tf.data.Dataset.zip((audio_ds, label_ds))\n\ndef audio_to_fft(audio):\n    \"\"\"\n    Since tf.signal.fft applies FFT on the innermost dimension,\n    we need to squeeze the dimensions and then expand them again\n    after FFT\n    \"\"\"\n    audio = tf.squeeze(audio, axis=-1)\n    fft = tf.signal.fft(\n                        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)),\n                                tf.complex64)\n                        )\n    fft = tf.expand_dims(fft, axis=-1)\n    # Return the absolute value of the first half of the FFT\n    # which represents the positive frequencies\n    return tf.math.abs(fft[:, : (int(DURATION * SAMPLING_RATE) // 2), :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = os.listdir(DATASET_AUDIO_PATH)\nprint(\"Our class names: {}\".format(class_names,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nwith open('class_names.pkl', 'wb') as f:\n    pickle.dump(class_names, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm './class_names.pkl'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TMP\n# Get the list of audio file paths along with their corresponding labels\n\naudio_paths, labels = get_audio_metadata()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle\nrng = np.random.RandomState(SHUFFLE_SEED)\nrng.shuffle(audio_paths)\nrng = np.random.RandomState(SHUFFLE_SEED)\nrng.shuffle(labels)\n\n# SPLIT into training and validation\nnum_val_samples = int(VALID_SPLIT * len(audio_paths))\n\nprint(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\ntrain_audio_paths = audio_paths[:-num_val_samples]\ntrain_labels = labels[:-num_val_samples]\n\nprint(\"Using {} files for validation.\".format(num_val_samples))\nvalid_audio_paths = audio_paths[-num_val_samples:]\nvalid_labels = labels[-num_val_samples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the list of audio file paths along with their corresponding labels\n\naudio_paths, labels = get_audio_metadata()\n\n# Shuffle\nrng = np.random.RandomState(SHUFFLE_SEED)\nrng.shuffle(audio_paths)\nrng = np.random.RandomState(SHUFFLE_SEED)\nrng.shuffle(labels)\n\n# SPLIT into training and validation\nnum_val_samples = int(VALID_SPLIT * len(audio_paths))\n\nprint(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\ntrain_audio_paths = audio_paths[:-num_val_samples]\ntrain_labels = labels[:-num_val_samples]\n\nprint(\"Using {} files for validation.\".format(num_val_samples))\nvalid_audio_paths = audio_paths[-num_val_samples:]\nvalid_labels = labels[-num_val_samples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Memory Optimization\ndel audio_paths\ndel labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE 2 DATASETS, one for training and the other for validation\ntrain_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\ntrain_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8,\n                            seed=SHUFFLE_SEED).batch(BATCH_SIZE)\nvalid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\nvalid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n\n# Transform audio wave to the frequency domain using `audio_to_fft`\ntrain_ds = train_ds.map(\n    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\ntrain_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n\nvalid_ds = valid_ds.map(\n    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\nvalid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.data.experimental.save(train_ds, \"./train_ds-tfrecord\", compression='GZIP') #, compression='GZIP'\n!zip -9 -r 'train_ds-tfrecord.zip' 'train_ds-tfrecord'\n!rm -rf ./train_ds-tfrecord\ntf.data.experimental.save(valid_ds, \"./valid_ds-tfrecord\", compression='GZIP') #, compression='GZIP'\n!zip -r 'valid_ds-tfrecord.zip' 'valid_ds-tfrecord'\n\n#from IPython.display import FileLink\n#FileLink(r'./valid_ds-tfrecord.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Carga de dataset tfrecords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"element_specs = (tf.TensorSpec(shape=(None, 40000, 1), dtype=tf.float32, name=None),  tf.TensorSpec(shape=(None,), dtype=tf.int32, name=None))\ntrain_ds = tf.data.experimental.load(\"../path/to/train_ds-tfrecord-3-9-2020/train_ds-tfrecord-3-9-2020\", element_specs, 'GZIP') #tf.TensorSpec(shape=((None, 40000, 1), (None,)), dtype=(tf.float32, tf.int32))\nvalid_ds = tf.data.experimental.load(\"../path/to/valid_ds-tfrecord-3-9-2020/valid_ds-tfrecord\", element_specs, 'GZIP') #tf.TensorSpec(shape=((None, 40000, 1), (None,)), dtype=(tf.float32, tf.int32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../path/to/class_names-3-9-2020.pkl', 'rb') as f:\n    class_names = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def residual_block(x, filters, conv_num=1, activation=\"relu\"): #3\n    # Shortcut\n    s = tf.keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n    for i in range(conv_num - 1):\n        x = tf.keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n        x = tf.keras.layers.Activation(activation)(x)\n    x = tf.keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n    x = tf.keras.layers.Add()([x, s])\n    x = tf.keras.layers.Activation(activation)(x)\n    return tf.keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n\n\ndef build_model(input_shape, num_classes):\n    inputs = tf.keras.layers.Input(shape=input_shape, name=\"input\")\n\n    x = residual_block(inputs, 16, 2)\n    x = residual_block(x, 32, 2)\n    x = residual_block(x, 64, 3)\n    x = residual_block(x, 128, 3)\n    x = residual_block(x, 128, 3)\n\n    x = tf.keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n\n    return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\n\nmodel = build_model((int(DURATION * SAMPLING_RATE) // 2, 1), len(class_names))\n\nmodel.summary()\n\n# Compile the model using Adam's default learning rate\n\nbase_learning_rate = 0.00001 #0.0001\nopt = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate)\n\nmodel.compile(\n    optimizer='rmsprop', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy']#[\"sparse_categorical_accuracy\"]\n)#Adam\n\n# Add callbacks:\n# 'EarlyStopping' to stop training when the model is not enhancing anymore\n# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\nmodel_save_filename = \"model.h5\"\n\nearlystopping_cb = tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\nmdlcheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n    model_save_filename, monitor=\"val_loss\", save_best_only=True\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_1d_conv_model(input_shape, num_classes):\n    \n    inp = tf.keras.layers.Input(shape=input_shape)\n    x = tf.keras.layers.Convolution1D(16, 9, activation=relu, padding=\"valid\")(inp)\n    x = tf.keras.layers.Convolution1D(16, 9, activation=relu, padding=\"valid\")(x)\n    x = tf.keras.layers.MaxPool1D(16)(x)\n    x = tf.keras.layers.Dropout(rate=0.1)(x)\n    \n    x = tf.keras.layers.Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n    x = tf.keras.layers.Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n    x = tf.keras.layers.MaxPool1D(4)(x)\n    x = tf.keras.layers.Dropout(rate=0.1)(x)\n    \n    x = tf.keras.layers.Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n    x = tf.keras.layers.Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n    x = tf.keras.layers.MaxPool1D(4)(x)\n    x = tf.keras.layers.Dropout(rate=0.1)(x)\n    \n    x = tf.keras.layers.Convolution1D(256, 3, activation=relu, padding=\"valid\")(x)\n    x = tf.keras.layers.Convolution1D(256, 3, activation=relu, padding=\"valid\")(x)\n    x = tf.keras.layers.GlobalMaxPool1D()(x)\n    x = tf.keras.layers.Dropout(rate=0.2)(x)\n\n    x = tf.keras.layers.Dense(64, activation=relu)(x)\n    x = tf.keras.layers.Dense(1028, activation=relu)(x)\n    out = tf.keras.layers.Dense(num_classes, activation=softmax)(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    #opt = optimizers.Adam(config.learning_rate)\n\n    return model\n\nmodel = build_model((int(DURATION * SAMPLING_RATE) // 2, 1), len(class_names))\nmodel.compile(optimizer='rmsprop', loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\n\n# Add callbacks:\n# 'EarlyStopping' to stop training when the model is not enhancing anymore\n# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\nmodel_save_filename = \"model.h5\"\n\nearlystopping_cb = tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\nmdlcheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n    model_save_filename, monitor=\"val_loss\", save_best_only=True\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    epochs=EPOCHS,\n    validation_data=valid_ds,\n    callbacks=[earlystopping_cb, mdlcheckpoint_cb], #\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.evaluate(valid_ds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**** 2D\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogMelSpectrogram(tf.keras.layers.Layer):\n    \"\"\"Compute log-magnitude mel-scaled spectrograms.\"\"\"\n\n    def __init__(self, sample_rate, fft_size, hop_size, n_mels,\n                 f_min=0.0, f_max=None, **kwargs):\n        super(LogMelSpectrogram, self).__init__(**kwargs)\n        self.sample_rate = sample_rate\n        self.fft_size = fft_size\n        self.hop_size = hop_size\n        self.n_mels = n_mels\n        self.f_min = f_min\n        self.f_max = f_max if f_max else sample_rate / 2\n        self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n            num_mel_bins=self.n_mels,\n            num_spectrogram_bins=fft_size // 2 + 1,\n            sample_rate=self.sample_rate,\n            lower_edge_hertz=self.f_min,\n            upper_edge_hertz=self.f_max)\n\n    def build(self, input_shape):\n        self.non_trainable_weights.append(self.mel_filterbank)\n        super(LogMelSpectrogram, self).build(input_shape)\n\n    def call(self, waveforms):\n        \"\"\"Forward pass.\n        Parameters\n        ----------\n        waveforms : tf.Tensor, shape = (None, n_samples)\n            A Batch of mono waveforms.\n        Returns\n        -------\n        log_mel_spectrograms : (tf.Tensor), shape = (None, time, freq, ch)\n            The corresponding batch of log-mel-spectrograms\n        \"\"\"\n        def _tf_log10(x):\n            numerator = tf.math.log(x)\n            denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n            return numerator / denominator\n\n        def power_to_db(magnitude, amin=1e-16, top_db=80.0):\n            \"\"\"\n            https://librosa.github.io/librosa/generated/librosa.core.power_to_db.html\n            \"\"\"\n            ref_value = tf.reduce_max(magnitude)\n            log_spec = 10.0 * _tf_log10(tf.maximum(amin, magnitude))\n            log_spec -= 10.0 * _tf_log10(tf.maximum(amin, ref_value))\n            log_spec = tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n\n            return log_spec\n\n        spectrograms = tf.signal.stft(waveforms,\n                                      frame_length=self.fft_size,\n                                      frame_step=self.hop_size,\n                                      pad_end=False)\n\n        magnitude_spectrograms = tf.abs(spectrograms)\n\n        mel_spectrograms = tf.matmul(tf.square(magnitude_spectrograms),\n                                     self.mel_filterbank)\n\n        log_mel_spectrograms = power_to_db(mel_spectrograms)\n\n        # add channel dimension\n        log_mel_spectrograms = tf.expand_dims(log_mel_spectrograms, 3)\n\n        return log_mel_spectrograms\n\n    def get_config(self):\n        config = {\n            'fft_size': self.fft_size,\n            'hop_size': self.hop_size,\n            'n_mels': self.n_mels,\n            'sample_rate': self.sample_rate,\n            'f_min': self.f_min,\n            'f_max': self.f_max,\n        }\n        config.update(super(LogMelSpectrogram, self).get_config())\n\n        return config\n\n_FFT_SIZE = 4096\n_HOP_SIZE = 86\n_N_MEL_BINS = 256\n\ndef ConvModel(n_classes, sample_rate=SAMPLING_RATE, duration=DURATION,\n              fft_size=_FFT_SIZE, hop_size=_HOP_SIZE, n_mels=_N_MEL_BINS):\n    n_samples = int(sample_rate * duration) // 2\n    \n    # Accept raw audio data as input\n    x = tf.keras.Input(shape=(n_samples,), name='input', dtype='float32')\n    # Process into log-mel-spectrograms. (This is your custom layer!)\n    y = LogMelSpectrogram(sample_rate, fft_size, hop_size, n_mels)(x)\n    # Normalize data (on frequency axis)\n    y = tf.keras.layers.BatchNormalization(axis=2)(y)\n    \n    y = tf.keras.layers.Conv2D(32, (3, n_mels), activation='relu')(y)\n    y = tf.keras.layers.BatchNormalization()(y)\n    y = tf.keras.layers.MaxPool2D((1, y.shape[2]))(y)\n\n    y = tf.keras.layers.Conv2D(32, (3, 1), activation='relu')(y)\n    y = tf.keras.layers.BatchNormalization()(y)\n    y = tf.keras.layers.MaxPool2D(pool_size=(2, 1))(y)\n\n    y = tf.keras.layers.Flatten()(y)\n    y = tf.keras.layers.Dense(64, activation='relu')(y)\n    y = tf.keras.layers.Dropout(0.25)(y)\n    y = tf.keras.layers.Dense(n_classes, activation='softmax')(y)\n\n    return tf.keras.Model(inputs=x, outputs=y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ConvModel(11)\nmodel.compile(optimizer=opt, \n              loss='sparse_categorical_crossentropy', \n              metrics=['sparse_categorical_accuracy'])\nmodel.summary()\n\n\nEPOCHS = 50\n\nhistory = model.fit(\n    train_ds,\n    epochs=EPOCHS,\n    validation_data=valid_ds,\n    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
